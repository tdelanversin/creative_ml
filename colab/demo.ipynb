{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdelanversin/creative_ml/blob/main/colab/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A two-stage U-Net for high-fidelity denoising of historical recordings"
      ],
      "metadata": {
        "id": "jbe_aWYkjWRH"
      },
      "id": "jbe_aWYkjWRH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a demo of the historical music denoising method proposed in:\n",
        "\n",
        "> E. Moliner and V. Välimäki,, \"A two-stage U-Net for high-fidelity denosing of historical recordings\", submitted to IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Singapore, May, 2022\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/64018465/131505025-e4530f55-fe5d-4bf4-ae64-cc9a502e5874.png\" alt=\"Schema represention\"\n",
        "width=\"400px\"></p>\n",
        "\n",
        "Listen to our [audio samples](http://research.spa.aalto.fi/publications/papers/icassp22-denoising/)\n",
        "\n",
        "You can freely use it to denoise your own historical recordings.\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n",
        "* For some reason, this notebook does not work in Firefox, so please use another browser.\n"
      ],
      "metadata": {
        "id": "8UON6ncSApA9"
      },
      "id": "8UON6ncSApA9"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Install and Import\n",
        "\n",
        "#@markdown Execute this cell to install the required data and dependencies. This step might take some time.\n",
        "\n",
        "#download the files\n",
        "! git clone https://github.com/eloimoliner/denoising-historical-recordings.git\n",
        "! wget https://github.com/eloimoliner/denoising-historical-recordings/releases/download/v0.0/checkpoint.zip\n",
        "! unzip checkpoint.zip -d denoising-historical-recordings/experiments/trained_model/\n",
        "\n",
        "%cd denoising-historical-recordings\n",
        "\n",
        "#install dependencies\n",
        "! pip install hydra-core==0.11.3\n",
        "\n",
        "#All the code goes here\n",
        "import unet\n",
        "import tensorflow as tf\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy.signal\n",
        "import hydra\n",
        "import os\n",
        "#workaround to load hydra conf file\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "args = yaml.safe_load(Path('conf/conf.yaml').read_text())\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "args=dotdict(args)\n",
        "unet_args=dotdict(args.unet)\n",
        "\n",
        "path_experiment=str(args.path_experiment)\n",
        "\n",
        "unet_model = unet.build_model_denoise(unet_args=unet_args)\n",
        "\n",
        "ckpt=os.path.join(\"/content/denoising-historical-recordings\",path_experiment, 'checkpoint')\n",
        "unet_model.load_weights(ckpt)\n",
        "\n",
        "def do_stft(noisy):\n",
        "\n",
        "    window_fn = tf.signal.hamming_window\n",
        "\n",
        "    win_size=args.stft[\"win_size\"]\n",
        "    hop_size=args.stft[\"hop_size\"]\n",
        "\n",
        "\n",
        "    stft_signal_noisy=tf.signal.stft(noisy,frame_length=win_size, window_fn=window_fn, frame_step=hop_size, pad_end=True)\n",
        "    stft_noisy_stacked=tf.stack( values=[tf.math.real(stft_signal_noisy), tf.math.imag(stft_signal_noisy)], axis=-1)\n",
        "\n",
        "    return stft_noisy_stacked\n",
        "\n",
        "def do_istft(data):\n",
        "\n",
        "    window_fn = tf.signal.hamming_window\n",
        "\n",
        "    win_size=args.stft[\"win_size\"]\n",
        "    hop_size=args.stft[\"hop_size\"]\n",
        "\n",
        "    inv_window_fn=tf.signal.inverse_stft_window_fn(hop_size, forward_window_fn=window_fn)\n",
        "\n",
        "    pred_cpx=data[...,0] + 1j * data[...,1]\n",
        "    pred_time=tf.signal.inverse_stft(pred_cpx, win_size, hop_size, window_fn=inv_window_fn)\n",
        "    return pred_time\n",
        "\n",
        "def denoise_audio(audio):\n",
        "\n",
        "    data, samplerate = sf.read(audio)\n",
        "    print(data.dtype)\n",
        "    #Stereo to mono\n",
        "    if len(data.shape)>1:\n",
        "        data=np.mean(data,axis=1)\n",
        "\n",
        "    if samplerate!=44100:\n",
        "        print(\"Resampling\")\n",
        "\n",
        "        data=scipy.signal.resample(data, int((44100  / samplerate )*len(data))+1)\n",
        "\n",
        "\n",
        "\n",
        "    segment_size=44100*5  #20s segments\n",
        "\n",
        "    length_data=len(data)\n",
        "    overlapsize=2048 #samples (46 ms)\n",
        "    window=np.hanning(2*overlapsize)\n",
        "    window_right=window[overlapsize::]\n",
        "    window_left=window[0:overlapsize]\n",
        "    audio_finished=False\n",
        "    pointer=0\n",
        "    denoised_data=np.zeros(shape=(len(data),))\n",
        "    residual_noise=np.zeros(shape=(len(data),))\n",
        "    numchunks=int(np.ceil(length_data/segment_size))\n",
        "\n",
        "    for i in tqdm(range(numchunks)):\n",
        "        if pointer+segment_size<length_data:\n",
        "            segment=data[pointer:pointer+segment_size]\n",
        "            #dostft\n",
        "            segment_TF=do_stft(segment)\n",
        "            segment_TF_ds=tf.data.Dataset.from_tensors(segment_TF)\n",
        "            pred = unet_model.predict(segment_TF_ds.batch(1))\n",
        "            pred=pred[0]\n",
        "            residual=segment_TF-pred[0]\n",
        "            residual=np.array(residual)\n",
        "            pred_time=do_istft(pred[0])\n",
        "            residual_time=do_istft(residual)\n",
        "            residual_time=np.array(residual_time)\n",
        "\n",
        "            if pointer==0:\n",
        "                pred_time=np.concatenate((pred_time[0:int(segment_size-overlapsize)], np.multiply(pred_time[int(segment_size-overlapsize):segment_size],window_right)), axis=0)\n",
        "                residual_time=np.concatenate((residual_time[0:int(segment_size-overlapsize)], np.multiply(residual_time[int(segment_size-overlapsize):segment_size],window_right)), axis=0)\n",
        "            else:\n",
        "                pred_time=np.concatenate((np.multiply(pred_time[0:int(overlapsize)], window_left), pred_time[int(overlapsize):int(segment_size-overlapsize)], np.multiply(pred_time[int(segment_size-overlapsize):int(segment_size)],window_right)), axis=0)\n",
        "                residual_time=np.concatenate((np.multiply(residual_time[0:int(overlapsize)], window_left), residual_time[int(overlapsize):int(segment_size-overlapsize)], np.multiply(residual_time[int(segment_size-overlapsize):int(segment_size)],window_right)), axis=0)\n",
        "\n",
        "            denoised_data[pointer:pointer+segment_size]=denoised_data[pointer:pointer+segment_size]+pred_time\n",
        "            residual_noise[pointer:pointer+segment_size]=residual_noise[pointer:pointer+segment_size]+residual_time\n",
        "\n",
        "            pointer=pointer+segment_size-overlapsize\n",
        "        else:\n",
        "            segment=data[pointer::]\n",
        "            lensegment=len(segment)\n",
        "            segment=np.concatenate((segment, np.zeros(shape=(int(segment_size-len(segment)),))), axis=0)\n",
        "            audio_finished=True\n",
        "            #dostft\n",
        "            segment_TF=do_stft(segment)\n",
        "\n",
        "            segment_TF_ds=tf.data.Dataset.from_tensors(segment_TF)\n",
        "\n",
        "            pred = unet_model.predict(segment_TF_ds.batch(1))\n",
        "            pred=pred[0]\n",
        "            residual=segment_TF-pred[0]\n",
        "            residual=np.array(residual)\n",
        "            pred_time=do_istft(pred[0])\n",
        "            pred_time=np.array(pred_time)\n",
        "            pred_time=pred_time[0:segment_size]\n",
        "            residual_time=do_istft(residual)\n",
        "            residual_time=np.array(residual_time)\n",
        "            residual_time=residual_time[0:segment_size]\n",
        "            if pointer==0:\n",
        "                pred_time=pred_time\n",
        "                residual_time=residual_time\n",
        "            else:\n",
        "                pred_time=np.concatenate((np.multiply(pred_time[0:int(overlapsize)], window_left), pred_time[int(overlapsize):int(segment_size)]),axis=0)\n",
        "                residual_time=np.concatenate((np.multiply(residual_time[0:int(overlapsize)], window_left), residual_time[int(overlapsize):int(segment_size)]),axis=0)\n",
        "\n",
        "            denoised_data[pointer::]=denoised_data[pointer::]+pred_time[0:lensegment]\n",
        "            residual_noise[pointer::]=residual_noise[pointer::]+residual_time[0:lensegment]\n",
        "    return denoised_data"
      ],
      "metadata": {
        "id": "TQBDTmO4mUBx",
        "outputId": "4428b0c3-2ba2-477a-e5a4-b56074e44450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "TQBDTmO4mUBx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'denoising-historical-recordings'...\n",
            "remote: Enumerating objects: 226, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects:   7% (1/13)\u001b[K\rremote: Compressing objects:  15% (2/13)\u001b[K\rremote: Compressing objects:  23% (3/13)\u001b[K\rremote: Compressing objects:  30% (4/13)\u001b[K\rremote: Compressing objects:  38% (5/13)\u001b[K\rremote: Compressing objects:  46% (6/13)\u001b[K\rremote: Compressing objects:  53% (7/13)\u001b[K\rremote: Compressing objects:  61% (8/13)\u001b[K\rremote: Compressing objects:  69% (9/13)\u001b[K\rremote: Compressing objects:  76% (10/13)\u001b[K\rremote: Compressing objects:  84% (11/13)\u001b[K\rremote: Compressing objects:  92% (12/13)\u001b[K\rremote: Compressing objects: 100% (13/13)\u001b[K\rremote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 226 (delta 14), reused 9 (delta 7), pack-reused 206 (from 1)\u001b[K\n",
            "Receiving objects: 100% (226/226), 116.56 KiB | 3.33 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "--2025-05-08 13:40:03--  https://github.com/eloimoliner/denoising-historical-recordings/releases/download/v0.0/checkpoint.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/401385223/354cec4e-d8be-4126-8b32-9e6509bca537?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250508T134003Z&X-Amz-Expires=300&X-Amz-Signature=a9bf4883a03d879a871ad01183ba43df2a955dbb2650f96962b574bf76f40e88&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcheckpoint.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-08 13:40:03--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/401385223/354cec4e-d8be-4126-8b32-9e6509bca537?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250508T134003Z&X-Amz-Expires=300&X-Amz-Signature=a9bf4883a03d879a871ad01183ba43df2a955dbb2650f96962b574bf76f40e88&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcheckpoint.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264031940 (252M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint.zip’\n",
            "\n",
            "checkpoint.zip      100%[===================>] 251.80M   170MB/s    in 1.5s    \n",
            "\n",
            "2025-05-08 13:40:05 (170 MB/s) - ‘checkpoint.zip’ saved [264031940/264031940]\n",
            "\n",
            "Archive:  checkpoint.zip\n",
            "  inflating: denoising-historical-recordings/experiments/trained_model/checkpoint  \n",
            "  inflating: denoising-historical-recordings/experiments/trained_model/checkpoint.data-00000-of-00001  \n",
            "  inflating: denoising-historical-recordings/experiments/trained_model/checkpoint.index  \n",
            "/content/denoising-historical-recordings/denoising-historical-recordings/denoising-historical-recordings\n",
            "Requirement already satisfied: hydra-core==0.11.3 in /usr/local/lib/python3.11/dist-packages (0.11.3)\n",
            "Requirement already satisfied: omegaconf<1.5,>=1.4 in /usr/local/lib/python3.11/dist-packages (from hydra-core==0.11.3) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from omegaconf<1.5,>=1.4->hydra-core==0.11.3) (6.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'd__block_27', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'd__block_25', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'sam_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'd__block_33', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'd__block_31', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'multi_stage_denoise_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File format not supported: filepath=/content/denoising-historical-recordings/experiments/trained_model/checkpoint.zip. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5565bbc7f1e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/denoising-historical-recordings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_experiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0munet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_stft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mlegacy_h5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0;34mf\"File format not supported: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m\"Keras 3 only supports V3 `.keras` and `.weights.h5` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=/content/denoising-historical-recordings/experiments/trained_model/checkpoint.zip. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Upload file to denoise\n",
        "\n",
        "#@markdown Execute this cell to upload a single audio recording you would like to denoise (accepted extensions: .wav, .flac, .mp3)\n",
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "50Kmdy6AtbhW"
      },
      "id": "50Kmdy6AtbhW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Denoise\n",
        "\n",
        "#@markdown Execute this cell to denoise the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "  print('Denoising uploaded file \"{name}\"'.format(\n",
        "      name=fn))\n",
        "  denoise_data=denoise_audio(fn)\n",
        "  basename=os.path.splitext(fn)[0]\n",
        "  wav_output_name=basename+\"_denoised\"+\".wav\"\n",
        "  sf.write(wav_output_name, denoise_data, 44100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "0po6zpvrylc2",
        "outputId": "173f5355-2939-41fe-c702-591aa752fc7e"
      },
      "id": "0po6zpvrylc2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Denoising uploaded file \"Carmen-Habanera_(Love_is_Like_a_Woo_-_Marguerite_D'Alvarez_noisy_input.wav\"\n",
            "float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:30<00:00,  1.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Download\n",
        "\n",
        "#@markdown Execute this cell to download the denoised recording\n",
        "files.download(wav_output_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "3tEshWBezYvf",
        "outputId": "54588c26-0b3c-42bf-aca2-8316ab54603f"
      },
      "id": "3tEshWBezYvf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3e3ca242-937f-408e-a16c-4aa5bb2b9e50\", \"Carmen-Habanera_(Love_is_Like_a_Woo_-_Marguerite_D'Alvarez_noisy_input_denoised.wav\", 17961334)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_FuSJ4J-WO-"
      },
      "id": "v_FuSJ4J-WO-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}